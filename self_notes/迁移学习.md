迁移学习中在相应的层注册hook函数

然后在hook函数中将对应层输出保存到全局的list中,例如下面代码在fc层前注册hook函数,可以将fc层的输出全部保存在cpu中,然后利用cpu运算全链接层,就不需要gpu了.

```python
in_list= [] # 这里存放所有的输出
def hook(module, input, output):
    #input是一个tuple代表顺序代表每一个输入项，我们这里只有一项，所以直接获取
    #需要全部的参数信息可以使用这个打印
    #for val in input:
    #    print("input val:",val)
    for i in range(input[0].size(0)):
        in_list.append(input[0][i].cpu().numpy())
```

